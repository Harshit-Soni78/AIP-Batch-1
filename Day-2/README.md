# üìå Natural Language Processing (NLP) Basics

## **1. Tokenization**

### **üîπ What is Tokenization?**

Tokenization is the process of breaking down text into smaller components, known as tokens. These tokens can be words, sentences, or subwords.

### **üîπ Types of Tokenization**

- **Word Tokenization:** Splitting text into individual words.
- **Sentence Tokenization:** Splitting text into sentences.

### **üìå Example: Word Tokenization**

#### **Input:**

```text
"AI is revolutionizing the world of technology! NLP is a key part of AI."
```

#### **Output:**

```text
['AI', 'is', 'revolutionizing', 'the', 'world', 'of', 'technology', '!', 'NLP', 'is', 'a', 'key', 'part', 'of', 'AI', '.']
```

### **üíª Code Example (NLTK - Word and Sentence Tokenization)**

```python
import nltk
from nltk.tokenize import word_tokenize, sent_tokenize

nltk.download('punkt')  # Download tokenizer models

text = "AI is revolutionizing the world of technology! NLP is a key part of AI."

# Word Tokenization
word_tokens = word_tokenize(text)
print("Word Tokens:", word_tokens)

# Sentence Tokenization
sentence_tokens = sent_tokenize(text)
print("Sentence Tokens:", sentence_tokens)
```

---

## **2. Stopwords Removal**

### **üîπ What are Stopwords?**

Stopwords are common words that do not add significant meaning to a sentence. Examples include _is, the, and, of, etc._

### **üìå Example: Stopword Removal**

#### **Input:**

```text
"This is an amazing NLP tutorial for beginners!"
```

#### **Output (After Removing Stopwords):**

```text
['amazing', 'NLP', 'tutorial', 'beginners', '!']
```

### **üíª Code Example (NLTK - Stopwords Removal)**

```python
from nltk.corpus import stopwords

nltk.download('stopwords')  # Download stopwords dataset

stop_words = set(stopwords.words('english'))

# Example text
tokens = word_tokenize("This is an amazing NLP tutorial for beginners!")

# Removing stopwords
filtered_tokens = [word for word in tokens if word.lower() not in stop_words] # used list comprehension of python

print("Original Tokens:", tokens)
print("Filtered Tokens (Stopwords removed):", filtered_tokens)
```

---

## **3. Stemming**

### **üîπ What is Stemming?**

Stemming reduces words to their base or root form by chopping off suffixes. The output may not always be a real word.

### **üìå Example: Stemming**

| Word      | Stemmed Form |
| --------- | ------------ |
| Running   | run          |
| Studies   | studi        |
| Beautiful | beauti       |

### **üíª Code Example (NLTK - Porter Stemmer)**

```python
from nltk.stem import PorterStemmer

stemmer = PorterStemmer()

words = ["running", "studies", "beautiful", "flies", "crying"]
stemmed_words = [stemmer.stem(word) for word in words]  # used list comprehension of python

print("Stemmed Words:", stemmed_words)
```

---

## **4. Lemmatization**

### **üîπ What is Lemmatization?**

Lemmatization is a more accurate way to get the base form of a word, considering its meaning and grammar.

### **üìå Example: Lemmatization**

| Word    | Lemmatized Form |
| ------- | --------------- |
| Running | run             |
| Studies | study           |
| Mice    | mouse           |
| Went    | go              |

### **üíª Code Example (NLTK - WordNet Lemmatizer)**

```python
from nltk.stem import WordNetLemmatizer

nltk.download('wordnet')  # Download dataset for lemmatization

lemmatizer = WordNetLemmatizer()

words = ["running", "studies", "mice", "went", "crying"]
lemmatized_words = [lemmatizer.lemmatize(word) for word in words]

print("Lemmatized Words:", lemmatized_words)
```

---

## **5. Stemming vs. Lemmatization**

| Feature | Stemming                         | Lemmatization              |
| ------- | -------------------------------- | -------------------------- |
| Output  | Can produce non-dictionary words | Always produces real words |
| Speed   | Faster                           | Slower                     |
| Example | "Studies" ‚Üí "Studi"              | "Studies" ‚Üí "Study"        |

### **üîπ When to Use What?**

‚úî **Stemming** is useful when speed is more important than accuracy.  
‚úî **Lemmatization** is preferred when meaning and correct grammar are required.

---

## **üéØ Conclusion**

1Ô∏è‚É£ **Tokenization** ‚Üí Splitting text into words/sentences.  
2Ô∏è‚É£ **Stopwords Removal** ‚Üí Removing common words that add no meaning.  
3Ô∏è‚É£ **Stemming** ‚Üí Cutting words to their root form (not always a real word).  
4Ô∏è‚É£ **Lemmatization** ‚Üí Converting words to meaningful root forms.

---

üöÄ **Next Steps:** Try applying these techniques to real-world text data and analyze how they impact NLP applications like sentiment analysis, chatbot responses, and text summarization!

## Author

**Harshit Soni**  
GitHub: [Harshit-Soni78](https://github.com/Harshit-Soni78)

---
Made with ‚ù§Ô∏è by Harshit Soni
