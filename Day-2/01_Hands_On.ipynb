{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìå Tokenization, Stopwords, Stemming & Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìù Goal: Break text into words, remove stopwords, and find word roots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: nltk in c:\\users\\harsh\\appdata\\roaming\\python\\python312\\site-packages (3.9.1)\n",
      "Requirement already satisfied: spacy in c:\\users\\harsh\\appdata\\roaming\\python\\python312\\site-packages (3.8.4)\n",
      "Requirement already satisfied: click in c:\\users\\harsh\\appdata\\roaming\\python\\python312\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\python312\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\harsh\\appdata\\roaming\\python\\python312\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\harsh\\appdata\\roaming\\python\\python312\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\harsh\\appdata\\roaming\\python\\python312\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\harsh\\appdata\\roaming\\python\\python312\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\harsh\\appdata\\roaming\\python\\python312\\site-packages (from spacy) (1.0.12)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\harsh\\appdata\\roaming\\python\\python312\\site-packages (from spacy) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\harsh\\appdata\\roaming\\python\\python312\\site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in c:\\users\\harsh\\appdata\\roaming\\python\\python312\\site-packages (from spacy) (8.3.4)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\harsh\\appdata\\roaming\\python\\python312\\site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\harsh\\appdata\\roaming\\python\\python312\\site-packages (from spacy) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\harsh\\appdata\\roaming\\python\\python312\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in c:\\users\\harsh\\appdata\\roaming\\python\\python312\\site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in c:\\users\\harsh\\appdata\\roaming\\python\\python312\\site-packages (from spacy) (0.15.1)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\python312\\lib\\site-packages (from spacy) (2.0.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\python312\\lib\\site-packages (from spacy) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\harsh\\appdata\\roaming\\python\\python312\\site-packages (from spacy) (2.10.6)\n",
      "Requirement already satisfied: jinja2 in c:\\python312\\lib\\site-packages (from spacy) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\harsh\\appdata\\roaming\\python\\python312\\site-packages (from spacy) (75.8.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\python312\\lib\\site-packages (from spacy) (24.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\harsh\\appdata\\roaming\\python\\python312\\site-packages (from spacy) (3.5.0)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\users\\harsh\\appdata\\roaming\\python\\python312\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\harsh\\appdata\\roaming\\python\\python312\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\harsh\\appdata\\roaming\\python\\python312\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.27.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in c:\\python312\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\python312\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\python312\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\python312\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\python312\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.8.30)\n",
      "Requirement already satisfied: blis<1.3.0,>=1.2.0 in c:\\users\\harsh\\appdata\\roaming\\python\\python312\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.2.0)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\harsh\\appdata\\roaming\\python\\python312\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
      "Requirement already satisfied: colorama in c:\\python312\\lib\\site-packages (from tqdm->nltk) (0.4.6)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\harsh\\appdata\\roaming\\python\\python312\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\python312\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in c:\\users\\harsh\\appdata\\roaming\\python\\python312\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.20.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\users\\harsh\\appdata\\roaming\\python\\python312\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\python312\\lib\\site-packages (from jinja2->spacy) (3.0.2)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in c:\\users\\harsh\\appdata\\roaming\\python\\python312\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\python312\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\python312\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
      "Requirement already satisfied: wrapt in c:\\python312\\lib\\site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.16.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\python312\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: Loading egg at c:\\python312\\lib\\site-packages\\vboxapi-1.0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Invalid requirement: '#': Expected package name at the start of dependency specifier\n",
      "    #\n",
      "    ^\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk spacy\n",
    "!pip install tf-keras # For text semerization pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "      --------------------------------------- 0.3/12.8 MB ? eta -:--:--\n",
      "     - -------------------------------------- 0.5/12.8 MB 2.1 MB/s eta 0:00:06\n",
      "     --- ------------------------------------ 1.0/12.8 MB 2.4 MB/s eta 0:00:05\n",
      "     ---- ----------------------------------- 1.6/12.8 MB 2.5 MB/s eta 0:00:05\n",
      "     ------- -------------------------------- 2.4/12.8 MB 2.7 MB/s eta 0:00:04\n",
      "     --------- ------------------------------ 3.1/12.8 MB 2.9 MB/s eta 0:00:04\n",
      "     ------------- -------------------------- 4.2/12.8 MB 3.3 MB/s eta 0:00:03\n",
      "     ----------------- ---------------------- 5.5/12.8 MB 3.7 MB/s eta 0:00:02\n",
      "     --------------------- ------------------ 6.8/12.8 MB 4.0 MB/s eta 0:00:02\n",
      "     ------------------------- -------------- 8.1/12.8 MB 4.3 MB/s eta 0:00:02\n",
      "     ----------------------------- ---------- 9.4/12.8 MB 4.5 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 11.5/12.8 MB 5.0 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 12.8/12.8 MB 5.1 MB/s eta 0:00:00\n",
      "\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: Loading egg at c:\\python312\\lib\\site-packages\\vboxapi-1.0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "  WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ConnectTimeoutError(<pip._vendor.urllib3.connection.HTTPSConnection object at 0x000001E2E1398830>, 'Connection to github.com timed out. (connect timeout=15)')': /explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl\n",
      "  WARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ConnectTimeoutError(<pip._vendor.urllib3.connection.HTTPSConnection object at 0x000001E2E1399130>, 'Connection to github.com timed out. (connect timeout=15)')': /explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl\n"
     ]
    }
   ],
   "source": [
    "# For Named Entity Recognition\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download required datasets and models\n",
    "nltk.download('punkt')        # For tokenization\n",
    "nltk.download('stopwords')    # For stopwords removal\n",
    "nltk.download('wordnet')      # For lemmatization\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download required datasets and models\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **üìå 1. Tokenization**\n",
    "\n",
    "### **What is Tokenization?**\n",
    "\n",
    "Tokenization is the process of splitting text into smaller parts, called **tokens**. These tokens can be words, sentences, or subwords.\n",
    "\n",
    "### **Example: Word Tokenization**\n",
    "\n",
    "üîπ **Text:**  \n",
    "_\"AI is revolutionizing the world of technology!\"_\n",
    "\n",
    "üîπ **Tokens:**  \n",
    "`['AI', 'is', 'revolutionizing', 'the', 'world', 'of', 'technology', '!']`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Tokens: ['AI', 'is', 'revolutionizing', 'the', 'world', 'of', 'technology', '!', 'NLP', 'is', 'a', 'key', 'part', 'of', 'AI', '.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "text = \"AI is revolutionizing the world of technology! NLP is a key part of AI.\"\n",
    "\n",
    "# Word Tokenization\n",
    "word_tokens = word_tokenize(text)\n",
    "print(\"Word Tokens:\", word_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence Tokens: ['You want to build ChatWithWebsite in Python, meaning the app should take a URL, modify it, and allow users to interact with the modified website.', \"Since your original project uses Next.js, let's translate its core functionality into a Flask or FastAPI-based Python web application.\", 'Plan for Python Version\\nBackend (Flask/FastAPI):\\n\\nAccepts a URL input.', 'Modifies the URL (http://localhost:5000/{your-url}).', 'Uses requests or selenium for fetching data from the target site.', 'Returns processed content to the frontend.', 'Frontend (HTML, CSS, JavaScript):\\n\\nSimple input form for entering a website URL.', 'Submits the URL to the backend.', 'Displays retrieved content or interactions.', 'Which Tech Stack Do You Prefer?', 'Flask (Lightweight, easy to deploy, good for small projects)\\nFastAPI (Asynchronous, high-performance, modern API handling)\\nDo you want to just redirect or fetch and display content?']\n"
     ]
    }
   ],
   "source": [
    "# Sentence Tokenization\n",
    "\n",
    "sentance = '''You want to build ChatWithWebsite in Python, meaning the app should take a URL, modify it, and allow users to interact with the modified website. Since your original project uses Next.js, let's translate its core functionality into a Flask or FastAPI-based Python web application.\n",
    "\n",
    "Plan for Python Version\n",
    "Backend (Flask/FastAPI):\n",
    "\n",
    "Accepts a URL input.\n",
    "Modifies the URL (http://localhost:5000/{your-url}).\n",
    "Uses requests or selenium for fetching data from the target site.\n",
    "Returns processed content to the frontend.\n",
    "Frontend (HTML, CSS, JavaScript):\n",
    "\n",
    "Simple input form for entering a website URL.\n",
    "Submits the URL to the backend.\n",
    "Displays retrieved content or interactions.\n",
    "Which Tech Stack Do You Prefer?\n",
    "Flask (Lightweight, easy to deploy, good for small projects)\n",
    "FastAPI (Asynchronous, high-performance, modern API handling)\n",
    "Do you want to just redirect or fetch and display content?'''\n",
    "\n",
    "sentence_tokens = sent_tokenize(sentance)\n",
    "print(\"Sentence Tokens:\", sentence_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìå 2. Stopwords Removal\n",
    "### What are Stopwords?\n",
    "Stopwords are common words (like is, the, and, of, etc.) that do not carry significant meaning and are often removed to improve NLP model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Tokens: ['This', 'is', 'an', 'amazing', 'NLP', 'tutorial', 'for', 'beginners', '!']\n",
      "Filtered Tokens (Stopwords removed): ['amazing', 'NLP', 'tutorial', 'beginners', '!']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "# nltk.download('stopwords')    \n",
    "\n",
    "stop_words = set(stopwords.words('english'))  # Load English stopwords\n",
    "\n",
    "# Example text\n",
    "tokens = word_tokenize(\"This is an amazing NLP tutorial for beginners!\")\n",
    "\n",
    "# Removing stopwords\n",
    "filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "\n",
    "print(\"Original Tokens:\", tokens)\n",
    "print(\"Filtered Tokens (Stopwords removed):\", filtered_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **üìå 3. Stemming**\n",
    "\n",
    "### **What is Stemming?**\n",
    "\n",
    "Stemming reduces a word to its **root or base form** by chopping off suffixes, even if the result is not a real word.\n",
    "\n",
    "üîπ **Example:**\n",
    "\n",
    "| Word | Stemmed Form |\n",
    "| --- | --- |\n",
    "| Running | run |\n",
    "| Studies | studi |\n",
    "| Beautiful | beauti |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmed Words: ['run', 'studi', 'beauti', 'fli', 'cri', 'coy', 'sere', 'suprem']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "words = [\"running\", \"studies\", \"beautiful\", \"flies\", \"crying\", \"coying\", \"sering\",\"supreme\"]\n",
    "stemmed_words = [stemmer.stem(word) for word in words]\n",
    "\n",
    "print(\"Stemmed Words:\", stemmed_words)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **üìå 4. Lemmatization**\n",
    "\n",
    "### **What is Lemmatization?**\n",
    "\n",
    "Lemmatization is similar to stemming but **returns actual words** instead of chopping off characters.\n",
    "\n",
    "üîπ **Example:**\n",
    "\n",
    "| Word | Lemmatized Form |\n",
    "| --- | --- |\n",
    "| Running | run |\n",
    "| Studies | study |\n",
    "| Mice | mouse |\n",
    "| Went | go |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized Words: ['running', 'study', 'mouse', 'went', 'cry']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "words = [\"running\", \"studies\", \"mice\", \"went\", \"crying\"]\n",
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "\n",
    "print(\"Lemmatized Words:\", lemmatized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['run', 'studi', 'mous', 'went', 'cri']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "stemmed_words = [stemmer.stem(word) for word in lemmatized_words]\n",
    "stemmed_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **üìå Key Differences: Stemming vs. Lemmatization**\n",
    "\n",
    "| Feature | Stemming | Lemmatization |\n",
    "| --- | --- | --- |\n",
    "| Output | Can produce non-dictionary words | Always produces real words |\n",
    "| Speed | Faster | Slower |\n",
    "| Example | \"Studies\" ‚Üí \"Studi\" | \"Studies\" ‚Üí \"Study\" |\n",
    "\n",
    "**‚û°Ô∏è When to use what?**  \n",
    "‚úî **Stemming** is useful when speed is more important than accuracy.  \n",
    "‚úî **Lemmatization** is preferred when meaning and correct grammar are required.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **üîπ Summary**\n",
    "\n",
    "1Ô∏è‚É£ **Tokenization** ‚Üí Splitting text into words/sentences.  \n",
    "2Ô∏è‚É£ **Stopwords Removal** ‚Üí Removing common words that add no meaning.  \n",
    "3Ô∏è‚É£ **Stemming** ‚Üí Cutting words to their root form (not always a real word).  \n",
    "4Ô∏è‚É£ **Lemmatization** ‚Üí Converting words to meaningful root forms.\n",
    "\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìå 5. Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìù Goal: Analyze text sentiment (Positive, Negative, Neutral)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "# nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment Scores: {'neg': 0.0, 'neu': 0.368, 'pos': 0.632, 'compound': 0.862}\n"
     ]
    }
   ],
   "source": [
    "sia = SentimentIntensityAnalyzer()\n",
    "text = \"I absolutely love this AI course! It's amazing.\"\n",
    "sentiment = sia.polarity_scores(text)\n",
    "\n",
    "print(\"Sentiment Scores:\", sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment Scores: {'neg': 0.425, 'neu': 0.575, 'pos': 0.0, 'compound': -0.8121}\n"
     ]
    }
   ],
   "source": [
    "sia = SentimentIntensityAnalyzer()\n",
    "text = \"I ordered the laptop but the delivery was of a pressure cooker. Totally disappointed.\"\n",
    "sentiment = sia.polarity_scores(text)\n",
    "\n",
    "print(\"Sentiment Scores:\", sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìå 6. Named Entity Recognition (NER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìù Goal: Identify names, dates, locations in text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_md')\n",
      "\u001b[38;5;3m‚ö† Restart to reload dependencies\u001b[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "spacy.cli.download(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elon Musk - PERSON\n",
      "SpaceX - ORG\n",
      "2002 - DATE\n",
      "Tesla - GPE\n",
      "2003 - DATE\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")  # Load the spaCy model\n",
    "\n",
    "text = \"Elon Musk founded SpaceX in 2002 and Tesla in 2003.\"\n",
    "doc = nlp(text)  # Process the text\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, \"-\" , ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'spacy.tokens.doc.Doc'>\n",
      "Spacexo - ORG\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "# text = \"Elon Musk founded AiPlaneTech in 2002 and Tesla in 2003 and SpaceX\"\n",
    "text = \"Spacexo is an organization\"\n",
    "doc = nlp(text)\n",
    "\n",
    "print(type(doc))\n",
    "\n",
    "# print(doc)\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, \"-\", ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìå 7. Build a Simple Chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## üìù Goal: Create a rule-based chatbot using NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "# from nltk.chat.util import Chat, reflections\n",
    "\n",
    "# pairs = [\n",
    "#     [\"hi|hello\", [\"Hello!\", \"Hi there!\"]],\n",
    "#     [\"how are you?\", [\"I'm a bot, I'm always good!\"]],\n",
    "#     [\"what's your name?\", [\"I'm a simple chatbot!\"]],\n",
    "# ]\n",
    "\n",
    "# chatbot = Chat(pairs, reflections)\n",
    "# chatbot.converse()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìå 8. Text Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìù Goal: Summarize long text using AI models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to google-t5/t5-small and revision df1b051 (https://huggingface.co/google-t5/t5-small).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\harsh\\AppData\\Roaming\\Python\\Python312\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\harsh\\AppData\\Roaming\\Python\\Python312\\site-packages\\tf_keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFT5ForConditionalGeneration.\n",
      "\n",
      "All the weights of TFT5ForConditionalGeneration were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n",
      "Device set to use 0\n"
     ]
    }
   ],
   "source": [
    "summarizer = pipeline(\"summarization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary: the ps command, which stands for ‚Äúprocess status,‚Äù is like a computer tool that helps you see what‚Äôs happening inside your Linux computer . you can make the ps command show you exactly what you want\n"
     ]
    }
   ],
   "source": [
    "text = \"The `ps` command, which stands for ‚Äúprocess status,‚Äù is like a computer tool that helps you see what‚Äôs happening inside your Linux computer. Imagine your computer is doing several things simultaneously, like running different programs or apps. These are the processes and the `ps` command lets you take a quick look at them. When you use it without any special instructions, it shows you the processes that are connected to the window or screen you are currently using. But here‚Äôs where it gets interesting: you can make the ps command show you exactly what you want to know by giving it special instructions, called options. These options let you customize the information you see, like finding out which programs are using the most computer power or checking what a specific user is doing. So, while it can give you a basic overview, the ps command‚Äôs real strength is in letting you choose exactly what details you want to see about the processes on your computer.\"\n",
    "summary = summarizer(text, max_length=50, min_length=20, do_sample=False)\n",
    "\n",
    "print(\"Summary:\", summary[0]['summary_text'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Projects",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
